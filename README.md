# Local LLM Chat with Ollama

이 프로젝트는 Ollama를 활용하여 로컬 환경에서 LLM(Large Language Model)을 실행하는 Next.js 기반 채팅 애플리케이션입니다.

## 프로젝트 개요

이 프로젝트는 다음과 같은 기능을 제공합니다:

- Ollama를 사용한 로컬 LLM 모델 실행
- 실시간 채팅 인터페이스
- 다양한 LLM 모델 선택 기능
- 채팅 기록 저장 및 관리

## 기술 스택

- Frontend: Next.js 14, React, TailwindCSS
- Backend: Next.js API Routes
- LLM: Ollama
- 기타: TypeScript

## 시작하기

### 사전 요구사항

1. [Ollama 설치](https://ollama.ai/download)
2. Node.js 18.0.0 이상
3. npm 또는 yarn

### 설치 방법

```bash
# 저장소 클론
git clone [your-repository-url]

# 의존성 설치
npm install
# 또는
yarn install

# 개발 서버 실행
npm run dev
# 또는
yarn dev
```

서버가 실행되면 [http://localhost:3000](http://localhost:3000)에서 애플리케이션을 확인할 수 있습니다.

## 주요 기능

- 💬 실시간 채팅 인터페이스
- 🤖 다양한 LLM 모델 지원
- 💾 채팅 기록 저장
- 🎨 사용자 친화적인 UI
- ⚡ 빠른 응답 속도

## 기여하기

프로젝트 기여는 언제나 환영합니다. 버그 리포트, 새로운 기능 제안 또는 풀 리퀘스트를 통해 참여해주세요.

## 라이선스

MIT License

## 문의사항

문의사항이 있으시면 이슈를 생성해주세요.
